{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "_FOLDER = \"data/\"\n",
    "_FOLDER_2 = \"results/\"\n",
    "_FOLDER_3 = \"datasets/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtration Scenarario 1 Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 225384\n",
      "Dataset size after initial filtration: 2776\n",
      "Dataset 1:\n",
      "  Size after filtration: 3667\n",
      "  Reduction from original dataset: 98.37%\n",
      "  Increase from initial filtration: 32.10%\n",
      "\n",
      "Dataset 2:\n",
      "  Size after filtration: 4543\n",
      "  Reduction from original dataset: 97.98%\n",
      "  Increase from initial filtration: 63.65%\n",
      "\n",
      "Dataset 3:\n",
      "  Size after filtration: 5436\n",
      "  Reduction from original dataset: 97.59%\n",
      "  Increase from initial filtration: 95.82%\n",
      "\n",
      "Dataset 4:\n",
      "  Size after filtration: 6308\n",
      "  Reduction from original dataset: 97.20%\n",
      "  Increase from initial filtration: 127.23%\n",
      "\n",
      "Dataset 5:\n",
      "  Size after filtration: 7145\n",
      "  Reduction from original dataset: 96.83%\n",
      "  Increase from initial filtration: 157.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Size of the dataset after the initial filtration (as provided)\n",
    "original_filtered_dataset_size = 2776\n",
    "original_dataset_reduction = 225384  # Initial dataset size before any filtration\n",
    "\n",
    "print(f\"Original dataset size: {original_dataset_reduction}\")\n",
    "print(f\"Dataset size after initial filtration: {original_filtered_dataset_size}\")\n",
    "\n",
    "# Loop through the datasets created in the first filtration scenario\n",
    "for i in range(1, 6):  # Assuming there are 5 datasets\n",
    "    dataset_path = _FOLDER_3 + f\"filtering_scenario_1.{i}.csv\"\n",
    "    filtered_dataset = pd.read_csv(dataset_path)\n",
    "    filtered_dataset_size = filtered_dataset.shape[0]\n",
    "    \n",
    "    # Calculate the reduction percentage from the original dataset\n",
    "    reduction_from_original = ((original_dataset_reduction - filtered_dataset_size) / original_dataset_reduction) * 100\n",
    "    \n",
    "    # Calculate the increase from the original filtered dataset\n",
    "    increase_from_original_filtered = ((filtered_dataset_size - original_filtered_dataset_size) / original_filtered_dataset_size) * 100\n",
    "    \n",
    "    print(f\"Dataset {i}:\")\n",
    "    print(f\"  Size after filtration: {filtered_dataset_size}\")\n",
    "    print(f\"  Reduction from original dataset: {reduction_from_original:.2f}%\")\n",
    "    print(f\"  Increase from initial filtration: {increase_from_original_filtered:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtration Scenario 2 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 225384\n",
      "Dataset size after initial filtration: 2776\n",
      "Dataset 1:\n",
      "  Size after filtration: 2956\n",
      "  Reduction from original dataset: 98.69%\n",
      "  Increase from initial filtration: 6.48%\n",
      "\n",
      "Dataset 2:\n",
      "  Size after filtration: 3125\n",
      "  Reduction from original dataset: 98.61%\n",
      "  Increase from initial filtration: 12.57%\n",
      "\n",
      "Dataset 3:\n",
      "  Size after filtration: 3276\n",
      "  Reduction from original dataset: 98.55%\n",
      "  Increase from initial filtration: 18.01%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataset after the initial filtration (as provided)\n",
    "original_filtered_dataset_size = 2776\n",
    "original_dataset_reduction = 225384  # Initial dataset size before any filtration\n",
    "\n",
    "print(f\"Original dataset size: {original_dataset_reduction}\")\n",
    "print(f\"Dataset size after initial filtration: {original_filtered_dataset_size}\")\n",
    "\n",
    "# Loop through the datasets created in the first filtration scenario\n",
    "for i in range(1, 4):  \n",
    "    dataset_path = _FOLDER_3 + f\"filtering_scenario_2.{i}.csv\"\n",
    "    filtered_dataset = pd.read_csv(dataset_path)\n",
    "    filtered_dataset_size = filtered_dataset.shape[0]\n",
    "    \n",
    "    # Calculate the reduction percentage from the original dataset\n",
    "    reduction_from_original = ((original_dataset_reduction - filtered_dataset_size) / original_dataset_reduction) * 100\n",
    "    \n",
    "    # Calculate the increase from the original filtered dataset\n",
    "    increase_from_original_filtered = ((filtered_dataset_size - original_filtered_dataset_size) / original_filtered_dataset_size) * 100\n",
    "    \n",
    "    print(f\"Dataset {i}:\")\n",
    "    print(f\"  Size after filtration: {filtered_dataset_size}\")\n",
    "    print(f\"  Reduction from original dataset: {reduction_from_original:.2f}%\")\n",
    "    print(f\"  Increase from initial filtration: {increase_from_original_filtered:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtration Scnerario 3 Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 225384\n",
      "Dataset size after initial filtration: 2776\n",
      "Dataset 1:\n",
      "  Size after filtration: 3902\n",
      "  Reduction from original dataset: 98.27%\n",
      "  Increase from initial filtration: 40.56%\n",
      "\n",
      "Dataset 2:\n",
      "  Size after filtration: 4144\n",
      "  Reduction from original dataset: 98.16%\n",
      "  Increase from initial filtration: 49.28%\n",
      "\n",
      "Dataset 3:\n",
      "  Size after filtration: 4356\n",
      "  Reduction from original dataset: 98.07%\n",
      "  Increase from initial filtration: 56.92%\n",
      "\n",
      "Dataset 4:\n",
      "  Size after filtration: 4848\n",
      "  Reduction from original dataset: 97.85%\n",
      "  Increase from initial filtration: 74.64%\n",
      "\n",
      "Dataset 5:\n",
      "  Size after filtration: 5153\n",
      "  Reduction from original dataset: 97.71%\n",
      "  Increase from initial filtration: 85.63%\n",
      "\n",
      "Dataset 6:\n",
      "  Size after filtration: 5436\n",
      "  Reduction from original dataset: 97.59%\n",
      "  Increase from initial filtration: 95.82%\n",
      "\n",
      "Dataset 7:\n",
      "  Size after filtration: 5788\n",
      "  Reduction from original dataset: 97.43%\n",
      "  Increase from initial filtration: 108.50%\n",
      "\n",
      "Dataset 8:\n",
      "  Size after filtration: 6171\n",
      "  Reduction from original dataset: 97.26%\n",
      "  Increase from initial filtration: 122.30%\n",
      "\n",
      "Dataset 9:\n",
      "  Size after filtration: 6534\n",
      "  Reduction from original dataset: 97.10%\n",
      "  Increase from initial filtration: 135.37%\n",
      "\n",
      "Dataset 10:\n",
      "  Size after filtration: 6733\n",
      "  Reduction from original dataset: 97.01%\n",
      "  Increase from initial filtration: 142.54%\n",
      "\n",
      "Dataset 11:\n",
      "  Size after filtration: 7196\n",
      "  Reduction from original dataset: 96.81%\n",
      "  Increase from initial filtration: 159.22%\n",
      "\n",
      "Dataset 12:\n",
      "  Size after filtration: 7645\n",
      "  Reduction from original dataset: 96.61%\n",
      "  Increase from initial filtration: 175.40%\n",
      "\n",
      "Dataset 13:\n",
      "  Size after filtration: 7655\n",
      "  Reduction from original dataset: 96.60%\n",
      "  Increase from initial filtration: 175.76%\n",
      "\n",
      "Dataset 14:\n",
      "  Size after filtration: 8194\n",
      "  Reduction from original dataset: 96.36%\n",
      "  Increase from initial filtration: 195.17%\n",
      "\n",
      "Dataset 15:\n",
      "  Size after filtration: 8747\n",
      "  Reduction from original dataset: 96.12%\n",
      "  Increase from initial filtration: 215.09%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Size of the dataset after the initial filtration (as provided)\n",
    "original_filtered_dataset_size = 2776\n",
    "original_dataset_reduction = 225384  # Initial dataset size before any filtration\n",
    "\n",
    "print(f\"Original dataset size: {original_dataset_reduction}\")\n",
    "print(f\"Dataset size after initial filtration: {original_filtered_dataset_size}\")\n",
    "\n",
    "# Loop through the datasets created in the first filtration scenario\n",
    "for i in range(1, 16):  \n",
    "    dataset_path = _FOLDER_3 + f\"filtering_scenario_3.{i}.csv\"\n",
    "    filtered_dataset = pd.read_csv(dataset_path)\n",
    "    filtered_dataset_size = filtered_dataset.shape[0]\n",
    "    \n",
    "    # Calculate the reduction percentage from the original dataset\n",
    "    reduction_from_original = ((original_dataset_reduction - filtered_dataset_size) / original_dataset_reduction) * 100\n",
    "    \n",
    "    # Calculate the increase from the original filtered dataset\n",
    "    increase_from_original_filtered = ((filtered_dataset_size - original_filtered_dataset_size) / original_filtered_dataset_size) * 100\n",
    "    \n",
    "    print(f\"Dataset {i}:\")\n",
    "    print(f\"  Size after filtration: {filtered_dataset_size}\")\n",
    "    print(f\"  Reduction from original dataset: {reduction_from_original:.2f}%\")\n",
    "    print(f\"  Increase from initial filtration: {increase_from_original_filtered:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis into the 3 filtering scenrarios showed that Scenario 1 caused a greater increase dataset than Scenario 2. When the limits of the first point and upper point are kept constant, every 0.01 increase in tolerence levels leads to a roughly 30% increase in the dataset. For Scenario 2, when the tolerence level is kept constant, every 0.05 increase in the lower platue and a decrease in the upper platue lead to a roughly 6% increase in the dataset.\n",
    "\n",
    "For Filtering Scenario 3, further analysis is required as it is a combination of the last two scenarios. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Analysis of Filtering Scneario 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tolerance: 0.06\n",
      "  First limit: 0.75, Last limit: 0.45, Dataset size: 3902, Increase from previous: 40.56%\n",
      "  First limit: 0.7, Last limit: 0.5, Dataset size: 4144, Increase from previous: 6.20%\n",
      "  First limit: 0.65, Last limit: 0.55, Dataset size: 4356, Increase from previous: 5.12%\n",
      "\n",
      "\n",
      "Tolerance: 0.07\n",
      "  First limit: 0.75, Last limit: 0.45, Dataset size: 4848, Increase from previous: 11.29%\n",
      "  First limit: 0.7, Last limit: 0.5, Dataset size: 5153, Increase from previous: 6.29%\n",
      "  First limit: 0.65, Last limit: 0.55, Dataset size: 5436, Increase from previous: 5.49%\n",
      "\n",
      "\n",
      "Tolerance: 0.08\n",
      "  First limit: 0.75, Last limit: 0.45, Dataset size: 5788, Increase from previous: 6.48%\n",
      "  First limit: 0.7, Last limit: 0.5, Dataset size: 6171, Increase from previous: 6.62%\n",
      "  First limit: 0.65, Last limit: 0.55, Dataset size: 6534, Increase from previous: 5.88%\n",
      "\n",
      "\n",
      "Tolerance: 0.09\n",
      "  First limit: 0.75, Last limit: 0.45, Dataset size: 6733, Increase from previous: 3.05%\n",
      "  First limit: 0.7, Last limit: 0.5, Dataset size: 7196, Increase from previous: 6.88%\n",
      "  First limit: 0.65, Last limit: 0.55, Dataset size: 7645, Increase from previous: 6.24%\n",
      "\n",
      "\n",
      "Tolerance: 0.1\n",
      "  First limit: 0.75, Last limit: 0.45, Dataset size: 7655, Increase from previous: 0.13%\n",
      "  First limit: 0.7, Last limit: 0.5, Dataset size: 8194, Increase from previous: 7.04%\n",
      "  First limit: 0.65, Last limit: 0.55, Dataset size: 8747, Increase from previous: 6.75%\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Parameters used in the third scenario\n",
    "tolerance_values = [0.06, 0.07, 0.08, 0.09, 0.10]\n",
    "first_points_lower_limits = [0.75, 0.7, 0.65]\n",
    "last_points_upper_limits = [0.45, 0.5, 0.55]\n",
    "\n",
    "# Original dataset sizes for comparison\n",
    "original_filtered_dataset_size = 2776\n",
    "original_dataset_size = 225384\n",
    "\n",
    "# Initialize a dictionary to store results\n",
    "tolerance_impact_details = {}\n",
    "\n",
    "previous_dataset_size = original_filtered_dataset_size\n",
    "\n",
    "# Process datasets\n",
    "scenario_counter = 1\n",
    "for tolerance_index, tolerance in enumerate(tolerance_values):\n",
    "    tolerance_impact_details[tolerance] = []\n",
    "    \n",
    "    # For each tolerance level after the first, reset the comparison base to the last dataset of the previous tolerance level\n",
    "    if tolerance_index > 0:\n",
    "        previous_dataset_size = tolerance_impact_details[tolerance_values[tolerance_index - 1]][-1][\"dataset_size\"]\n",
    "    \n",
    "    for first_limit, last_limit in zip(first_points_lower_limits, last_points_upper_limits):\n",
    "        dataset_path = _FOLDER_3 + f'filtering_scenario_3.{scenario_counter}.csv'\n",
    "        filtered_dataset = pd.read_csv(dataset_path)\n",
    "        filtered_dataset_size = filtered_dataset.shape[0]\n",
    "        \n",
    "        # Calculate the percentage increase from the previous dataset size\n",
    "        if scenario_counter == 1 or (first_limit == first_points_lower_limits[0] and tolerance == tolerance_values[0]):\n",
    "            # For the very first dataset of the entire series, compare against the original filtered dataset size\n",
    "            increase_from_previous = ((filtered_dataset_size - original_filtered_dataset_size) / original_filtered_dataset_size) * 100\n",
    "        else:\n",
    "            # For all other datasets, compare against the last dataset size\n",
    "            increase_from_previous = ((filtered_dataset_size - previous_dataset_size) / previous_dataset_size) * 100\n",
    "        \n",
    "        # Update for the next iteration\n",
    "        previous_dataset_size = filtered_dataset_size\n",
    "        \n",
    "        # Store results\n",
    "        tolerance_impact_details[tolerance].append({\n",
    "            \"first_limit\": first_limit,\n",
    "            \"last_limit\": last_limit,\n",
    "            \"dataset_size\": filtered_dataset_size,\n",
    "            \"increase_from_previous\": increase_from_previous\n",
    "        })\n",
    "        \n",
    "        scenario_counter += 1\n",
    "\n",
    "# Print out the detailed analysis results\n",
    "for tolerance, details in tolerance_impact_details.items():\n",
    "    print(f\"Tolerance: {tolerance}\")\n",
    "    for detail in details:\n",
    "        print(f\"  First limit: {detail['first_limit']}, Last limit: {detail['last_limit']}, Dataset size: {detail['dataset_size']}, Increase from previous: {detail['increase_from_previous']:.2f}%\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some interesting dicovering were made. At lower tolerence levels change the upper and lower platues provided a lower change than at higher tolerence levels. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
